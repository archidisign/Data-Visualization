{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspired by the work of https://github.com/hardikvasa/wikipedia-crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time     #For Delay\n",
    "import urllib.request    #Extracting web pages\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading entire Web Document (Raw Page Content)\n",
    "def download_page(url):\n",
    "    try:\n",
    "        headers = {}\n",
    "        headers['User-Agent'] = \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17\"\n",
    "        req = urllib.request.Request(url, headers = headers)\n",
    "        resp = urllib.request.urlopen(req)\n",
    "        respData = str(resp.read())\n",
    "        return respData\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the title tag\n",
    "def extract_info(page, start_tag, end_tag):\n",
    "    start = page.find(start_tag)\n",
    "    end = page.find(end_tag, start + 1)\n",
    "    n = len(start_tag)\n",
    "    info = page[start + n : end]\n",
    "    return info, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all the links\n",
    "#Finding 'Next Link' on a given web page\n",
    "def get_next_link(s):\n",
    "    start_link = s.find(\"<a href\")\n",
    "    if start_link == -1:    #If no links are found then give an error!\n",
    "        end_quote = 0\n",
    "        link = \"no_links\"\n",
    "        return link, end_quote\n",
    "    else:\n",
    "        start_quote = s.find('\"', start_link)\n",
    "        end_quote = s.find('\"',start_quote+1)\n",
    "        link = str(s[start_quote+1:end_quote])\n",
    "        return link, end_quote\n",
    "\n",
    "#Getting all links with the help of 'get_next_links'\n",
    "def get_all_links(page):\n",
    "    links = []\n",
    "    while True:\n",
    "        link, end_link = get_next_link(page)\n",
    "        if link == \"no_links\":\n",
    "            break\n",
    "        else:\n",
    "            links.append(link)      #Append all the links in the list named 'Links'\n",
    "            #time.sleep(0.1)\n",
    "            page = page[end_link:]\n",
    "    return links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Season Appearance\n",
    "def extract_season(page):\n",
    "    season_path = seed_page + get_next_link(page[page.find('First appearance'):])[0]\n",
    "    season = extract_info(download_page(season_path), 'Season', ',')[0]\n",
    "    return re.sub(\"[^0-9]\", \"\", season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract just the Introduction part of the page\n",
    "def extract_introduction(page):\n",
    "    start_introduction = page.find(\"<meta property=\\\"og:description\\\" content=\")\n",
    "    stop_introduction = page.find('<div id=\"toctitle\">', start_introduction + 1)\n",
    "    #If the page onl has introduction\n",
    "    if '<div id=\"toctitle\">' not in page:\n",
    "        stop_introduction = page.find('</p>', start_introduction + 1)\n",
    "    else:\n",
    "        pass    \n",
    "    raw_introduction = page[start_introduction : stop_introduction]\n",
    "    return raw_introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_page(\"http://gossipgirl.wikia.com/wiki/Dan_Humphrey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all the HTML tags from the introduction to get the pure text\n",
    "#Eliminate all the text inside '<' & '>'\n",
    "def extract_pure_text(page):\n",
    "    pure_text = (re.sub(r'<.+?>', '', page))       #From '<' to the next '>'\n",
    "    return pure_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crawl Initiation\n",
    "#Check for file type in URL so crawler does not crawl images and text files\n",
    "def extension_scan(url):\n",
    "    a = ['.png','.jpg','.jpeg','.gif','.tif','.txt']\n",
    "    j = 0\n",
    "    while j < (len(a)):\n",
    "        if a[j] in url:\n",
    "            #print(\"There!\")\n",
    "            flag2 = 1\n",
    "            break\n",
    "        else:\n",
    "            #print(\"Not There!\")\n",
    "            flag2 = 0\n",
    "            j = j+1\n",
    "    #print(flag2)\n",
    "    return flag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL parsing for incomplete or duplicate URLs\n",
    "def url_parse(url):\n",
    "    try:\n",
    "        from urllib.parse import urlparse\n",
    "    except ImportError:\n",
    "        from urlparse import urlparse\n",
    "    url = url  #.lower()    #Make it lower case\n",
    "    s = urlparse(url)       #parse the given url\n",
    "    seed_page_n = seed_page #.lower()       #Make it lower case\n",
    "    #t = urlparse(seed_page_n)     #parse the seed page (reference page)\n",
    "    i = 0\n",
    "    flag = 0\n",
    "    while i<=9:\n",
    "        if url == \"/\":\n",
    "            url = seed_page_n\n",
    "            flag = 0  \n",
    "        elif not s.scheme:\n",
    "            url = \"http://\" + url\n",
    "            flag = 0\n",
    "        elif \"#\" in url:\n",
    "            url = url[:url.find(\"#\")]\n",
    "            flag = 0\n",
    "        elif \"?\" in url:\n",
    "            url = url[:url.find(\"?\")]\n",
    "            flag = 0\n",
    "        elif s.netloc == \"\":\n",
    "            url = seed_page + s.path\n",
    "            flag = 0\n",
    "        elif \"www\" not in url:\n",
    "            url = \"www.\"[:7] + url[7:]\n",
    "            flag = 0\n",
    "        elif url[len(url)-1] == \"/\":\n",
    "            url = url[:-1]\n",
    "            flag = 0     \n",
    "        else:\n",
    "            url = url\n",
    "            flag = 0\n",
    "            break\n",
    "        \n",
    "        i = i+1\n",
    "        s = urlparse(url)   #Parse after every loop to update the values of url parameters\n",
    "    return(url, flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract for family\n",
    "def find_relations(page, source_name, start_tag, end_tag, nodes, links, degree, visited):\n",
    "    start = page.find(start_tag)\n",
    "    end = page[start:].find(end_tag)\n",
    "    i = start\n",
    "    while(i < start+end):\n",
    "        link, j = get_next_link(page[i:])\n",
    "        name = extract_info(page[i+j:], \">\", \"<\")[0]\n",
    "        if name not in visited:\n",
    "            relation, k = extract_info(page[i+j:], \"(\", \")\")\n",
    "            i += k+j\n",
    "            relation = extract_pure_text(relation)\n",
    "            nodes += [{'link': url_parse(link)[0], 'name': name, 'relation': relation, 'degree': degree+1}]\n",
    "            links += [{'source': source_name, 'target': name, 'relation': relation}]\n",
    "\n",
    "def find_people(page, source_name, start_tag, end_tag, relation, nodes, links, degree, visited):\n",
    "    start = page.find(start_tag)\n",
    "    end = page[start:].find(end_tag)\n",
    "    i = start\n",
    "    while(i < start+end):\n",
    "        link, j = get_next_link(page[i:])\n",
    "        name, k = extract_info(page[i+j:], \">\", \"<\")\n",
    "        if name not in visited:\n",
    "            i += k+j\n",
    "            nodes += [{'link': url_parse(link)[0], 'name': name, 'relation': relation, 'degree': degree+1}]\n",
    "            links += [{'source': source_name, 'target': name, 'relation': relation}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "one character done\n",
      "<urlopen error [Errno 11001] getaddrinfo failed>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-66159b47145c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Extract season\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'season'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_season\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;31m# Short Description (color of line)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'desc'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_pure_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_introduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-29ae7fb4798b>\u001b[0m in \u001b[0;36mextract_season\u001b[1;34m(page)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_season\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mseason_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseed_page\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mget_next_link\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'First appearance'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mseason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownload_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseason_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Season'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[^0-9]\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseason\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-dc82d84669c1>\u001b[0m in \u001b[0;36mextract_info\u001b[1;34m(page, start_tag, end_tag)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Extract the title tag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_tag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_tag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_tag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_tag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_tag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "#Defining pages\n",
    "starting_page = \"http://gossipgirl.wikia.com/wiki/Serena_van_der_Woodsen\"\n",
    "seed_page = \"http://gossipgirl.wikia.com\"  #Crawling the English Wikipedia\n",
    "\n",
    "char_dict = {}\n",
    "links = []\n",
    "visited = []\n",
    "iterator=0\n",
    "\n",
    "# URL\n",
    "char_dict['link'] = starting_page\n",
    "# Degree\n",
    "char_dict['degree'] = 0\n",
    "degree = 0\n",
    "# Name\n",
    "page = download_page(starting_page)\n",
    "start_tag = r'<h1 class=\"page-header__title\">'\n",
    "end_tag = \"</h1>\"\n",
    "char_dict['name'] = extract_info(page, start_tag, end_tag)[0]\n",
    "nodes = [char_dict]\n",
    "visited += char_dict['name']\n",
    "\n",
    "while(iterator<len(nodes)):\n",
    "    page = download_page(nodes[iterator]['link'])\n",
    "    name = nodes[iterator]['name']\n",
    "    if nodes[iterator]['degree'] > 6:\n",
    "        break\n",
    "    # Extract season\n",
    "    nodes[iterator]['season'] = extract_season(page)\n",
    "    # Short Description (color of line)\n",
    "    nodes[iterator]['desc'] = extract_pure_text(extract_introduction(page))\n",
    "    \n",
    "    # Find relationships\n",
    "    find_relations(page, name, \"Family</b></h3>\", \"Romances</b></h3>\", nodes, links, nodes[iterator]['degree'], visited)\n",
    "    # Extract for Romances\n",
    "    find_relations(page, name, \"Romances</b></h3>\", \"Friends</b></h3>\", nodes, links, nodes[iterator]['degree'], visited)\n",
    "    # Extract for friends\n",
    "    find_people(page, name, \"Friends</b></h3>\", \"Enemies</b></h3>\", 'Friend', nodes, links, nodes[iterator]['degree'], visited)\n",
    "    # Extract for Enemies\n",
    "    find_people(page, name, \"Enemies</b></h3>\", \"</section>\", 'Enemy', nodes, links, nodes[iterator]['degree'], visited)\n",
    "    \n",
    "    print('one character done')    \n",
    "    iterator += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = {}   #Create a dictionary\n",
    "\n",
    "#Main Crawl function that calls all the above function and crawls the entire site sequentially\n",
    "def web_crawl():  \n",
    "    to_crawl = [starting_page]      #Define list name 'Seed Page'\n",
    "    #print(to_crawl)\n",
    "    crawled=[]      #Define list name 'Seed Page'\n",
    "    #database = {}   #Create a dictionary\n",
    "    #k = 0;\n",
    "    for k in range(0, 3):\n",
    "        i=0        #Initiate Variable to count No. of Iterations\n",
    "        while i<3:     #Continue Looping till the 'to_crawl' list is not empty\n",
    "            urll = to_crawl.pop(0)      #If there are elements in to_crawl then pop out the first element\n",
    "            urll,flag = url_parse(urll)\n",
    "            #print(urll)\n",
    "            flag2 = extension_scan(urll)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            #If flag = 1, then the URL is outside the seed domain URL\n",
    "            if flag == 1 or flag2 == 1:\n",
    "                pass        #Do Nothing\n",
    "                \n",
    "            else:       \n",
    "                if urll in crawled:     #Else check if the URL is already crawled\n",
    "                    pass        #Do Nothing\n",
    "                else:       #If the URL is not already crawled, then crawl i and extract all the links from it\n",
    "                    print(\"Link = \" + urll)\n",
    "                    \n",
    "                    raw_html = download_page(urll)\n",
    "                    #print(raw_html)\n",
    "                    \n",
    "                    title_upper = str(extract_title(raw_html))\n",
    "                    title = title_upper.lower()     #Lower title to match user queries\n",
    "                    print(\"Title = \" + title)\n",
    "                     \n",
    "                    raw_introduction = extract_introduction(raw_html)\n",
    "                    #print(\"Raw Introduction = \" + raw_introduction)\n",
    "                    \n",
    "                    to_crawl = to_crawl + get_all_links(raw_introduction)\n",
    "                    crawled.append(urll)\n",
    "                    \n",
    "                    pure_introduction = extract_pure_text(raw_introduction)\n",
    "                    print(\"Introduction = \" + pure_introduction.replace('   ',' '))\n",
    "                    \n",
    "                    database [title] = pure_introduction        #Add title and its introduction to the dict\n",
    "                    \n",
    "                    #Writing the output data into a text file\n",
    "                    file = open('database.txt', 'a')        #Open the text file called database.txt\n",
    "                    file.write(title + \": \" + \"\\n\")         #Write the title of the page\n",
    "                    file.write(pure_introduction + \"\\n\\n\")      #write the introduction of that page\n",
    "                    file.close()                            #Close the file\n",
    "                    \n",
    "    \n",
    "                    #Remove duplicated from to_crawl\n",
    "                    n = 1\n",
    "                    j = 0\n",
    "                    #k = 0\n",
    "                    while j < (len(to_crawl)-n):\n",
    "                        if to_crawl[j] in to_crawl[j+1:(len(to_crawl)-1)]:\n",
    "                            to_crawl.pop(j)\n",
    "                            n = n+1\n",
    "                        else:\n",
    "                            pass     #Do Nothing\n",
    "                        j = j+1\n",
    "                i=i+1\n",
    "                print(i)\n",
    "                print(k)\n",
    "                #print(to_crawl)\n",
    "                #print(\"Iteration No. = \" + str(i))\n",
    "                #print(\"To Crawl = \" + str(len(to_crawl)))\n",
    "                #print(\"Crawled = \" + str(len(crawled)))\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
