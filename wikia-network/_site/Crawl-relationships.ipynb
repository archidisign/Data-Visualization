{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this project was inpired by the Wikipedia Crawler found here: https://github.com/hardikvasa/wikipedia-crawler\n",
    "My project is more specific to Wikia pages (huge fandom-based website and the goal was to look at specific characters' links with each other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading entire Web Document (Raw Page Content)\n",
    "def download_page(url):\n",
    "    try:\n",
    "        headers = {}\n",
    "        headers['User-Agent'] = \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17\"\n",
    "        req = urllib.request.Request(url, headers = headers)\n",
    "        resp = urllib.request.urlopen(req)\n",
    "        respData = str(resp.read())\n",
    "        return respData\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the title tag\n",
    "def extract_info(page, start_tag, end_tag):\n",
    "    start = page.find(start_tag)\n",
    "    if start == -1:\n",
    "        return '', -1\n",
    "    else:\n",
    "        end = page.find(end_tag, start + 1)\n",
    "        n = len(start_tag)\n",
    "        info = page[start + n : end]\n",
    "        return info, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all the links\n",
    "#Finding 'Next Link' on a given web page\n",
    "def get_next_link(s):\n",
    "    start_link = s.find(\"<a href\")\n",
    "    if start_link == -1:    #If no links are found then give an error!\n",
    "        end_quote = 0\n",
    "        link = \"no_links\"\n",
    "        return link, end_quote\n",
    "    else:\n",
    "        start_quote = s.find('\"', start_link)\n",
    "        end_quote = s.find('\"',start_quote+1)\n",
    "        link = str(s[start_quote+1:end_quote])\n",
    "        return link, end_quote\n",
    "\n",
    "#Getting all links with the help of 'get_next_links'\n",
    "def get_all_links(page):\n",
    "    links = []\n",
    "    while True:\n",
    "        link, end_link = get_next_link(page)\n",
    "        if link == \"no_links\":\n",
    "            break\n",
    "        else:\n",
    "            links.append(link)      #Append all the links in the list named 'Links'\n",
    "            page = page[end_link:]\n",
    "    return links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Season Appearance\n",
    "def extract_season(page):\n",
    "    try:\n",
    "        season_path = seed_page + get_next_link(page[page.find('First appearance'):])[0]\n",
    "        season = extract_info(download_page(season_path), 'Season', ',')[0]\n",
    "        return re.sub(\"[^0-9]\", \"\", season)\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all the HTML tags from a paragraph of text\n",
    "def extract_pure_text(page):\n",
    "    pure_text = (re.sub(r'<.+?>', '', page))\n",
    "    return pure_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crawl Initiation\n",
    "#Check for file type in URL so crawler does not crawl images and text files\n",
    "def extension_scan(url):\n",
    "    a = ['.png','.jpg','.jpeg','.gif','.tif','.txt']\n",
    "    j = 0\n",
    "    while j < (len(a)):\n",
    "        if a[j] in url:\n",
    "            #print(\"There!\")\n",
    "            flag2 = 1\n",
    "            break\n",
    "        else:\n",
    "            #print(\"Not There!\")\n",
    "            flag2 = 0\n",
    "            j = j+1\n",
    "    #print(flag2)\n",
    "    return flag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL parsing for incomplete or duplicate URLs\n",
    "def url_parse(url):\n",
    "    try:\n",
    "        from urllib.parse import urlparse\n",
    "    except ImportError:\n",
    "        from urlparse import urlparse\n",
    "    url = url  #.lower()    #Make it lower case\n",
    "    s = urlparse(url)       #parse the given url\n",
    "    seed_page_n = seed_page #.lower()       #Make it lower case\n",
    "    #t = urlparse(seed_page_n)     #parse the seed page (reference page)\n",
    "    i = 0\n",
    "    flag = 0\n",
    "    while i<=9:\n",
    "        if url == \"/\":\n",
    "            url = seed_page_n\n",
    "            flag = 0  \n",
    "        elif not s.scheme:\n",
    "            url = \"http://\" + url\n",
    "            flag = 0\n",
    "        elif \"#\" in url:\n",
    "            url = url[:url.find(\"#\")]\n",
    "            flag = 0\n",
    "        elif \"?\" in url:\n",
    "            url = url[:url.find(\"?\")]\n",
    "            flag = 0\n",
    "        elif s.netloc == \"\":\n",
    "            url = seed_page + s.path\n",
    "            flag = 0\n",
    "        elif \"www\" not in url:\n",
    "            url = \"www.\"[:7] + url[7:]\n",
    "            flag = 0\n",
    "        elif url[len(url)-1] == \"/\":\n",
    "            url = url[:-1]\n",
    "            flag = 0     \n",
    "        else:\n",
    "            url = url\n",
    "            flag = 0\n",
    "            break\n",
    "        \n",
    "        i = i+1\n",
    "        s = urlparse(url)\n",
    "    return(url, flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relations(page, source_name, start_tag, end_tag, nodes, links, degree, visited):\n",
    "    start = page.find(start_tag)\n",
    "    end = page[start:].find(end_tag)\n",
    "    i = start\n",
    "    while i < start+end:\n",
    "        if start == -1 or end == -1:\n",
    "            break\n",
    "        link, j = get_next_link(page[i:])        \n",
    "        name = extract_info(page[i+j:], \">\", \"<\")[0]\n",
    "        flag = True\n",
    "        for node in nodes:\n",
    "            if node['link'] == url_parse(link)[0]:\n",
    "                flag = False\n",
    "        \n",
    "        relation, k = extract_info(page[i+j:], \"(\", \")\")\n",
    "        relation = extract_pure_text(relation)\n",
    "        \n",
    "        if relation == \"\":\n",
    "            break\n",
    "        elif k != -1:\n",
    "            i += k+j\n",
    "            if flag:\n",
    "                nodes += [{'link': url_parse(link)[0], 'name': name, 'degree': degree+1}]\n",
    "            if link not in visited and link != \"no_links\":\n",
    "                links += [{'source': source_name, 'target': name, 'relation': relation}]\n",
    "                visited += [link]\n",
    "        else:\n",
    "            i += j\n",
    "\n",
    "def find_people(page, source_name, start_tag, end_tag, relation, nodes, links, degree, visited):\n",
    "    start = page.find(start_tag)\n",
    "    end = page[start:].find(end_tag)\n",
    "    i = start\n",
    "    while i < start+end:\n",
    "        if start == -1 or end == -1:\n",
    "            break\n",
    "        link, j = get_next_link(page[i:])\n",
    "        name, k = extract_info(page[i+j:], \">\", \"<\")\n",
    "        flag = True\n",
    "        for node in nodes:\n",
    "            if node['link'] == url_parse(link)[0]:\n",
    "                flag = False\n",
    "                \n",
    "        i += k+j\n",
    "        if flag:\n",
    "            nodes += [{'link': url_parse(link)[0], 'name': name, 'degree': degree+1}]\n",
    "        if link not in visited and link != \"no_links\":\n",
    "            links += [{'source': source_name, 'target': name, 'relation': relation}]\n",
    "            visited += [link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Defining pages\n",
    "starting_page = \"http://gossipgirl.wikia.com/wiki/Serena_van_der_Woodsen\"\n",
    "seed_page = \"http://www.gossipgirl.wikia.com\"  #Crawling the English Wikipedia\n",
    "\n",
    "char_dict = {}\n",
    "links = []\n",
    "visited = []\n",
    "iterator=0\n",
    "\n",
    "# URL\n",
    "char_dict['link'] = starting_page\n",
    "# Degree\n",
    "char_dict['degree'] = 0\n",
    "degree = 0\n",
    "# Name\n",
    "page = download_page(starting_page)\n",
    "char_dict['name'] = extract_info(page, r'<h1 class=\"page-header__title\">', \"</h1>\")[0]\n",
    "nodes = [char_dict]\n",
    "visited += [char_dict['link']]\n",
    "\n",
    "while(degree <= 1):\n",
    "    page = download_page(nodes[iterator]['link'])\n",
    "    name = nodes[iterator]['name']\n",
    "    # Extract season\n",
    "    nodes[iterator]['season'] = extract_season(page)\n",
    "    # Define degree\n",
    "    degree=nodes[iterator]['degree']\n",
    "    \n",
    "    # Find relationships\n",
    "    try:\n",
    "        page=page[:extract_info(page, \"<span>Relationships</span>\", \"Series information</h2>\")[1]]\n",
    "        # Extract for family\n",
    "        find_relations(page, name, \"<b>Family</b></h3>\", \"<b>Romances</b></h3>\", nodes, links, degree, visited)\n",
    "        # Extract for Romances\n",
    "        find_relations(page, name, \"<b>Romances</b></h3>\", \"<b>Friends</b></h3>\", nodes, links, degree, visited)\n",
    "        # Extract for friends\n",
    "        find_people(page, name, \"<b>Friends</b></h3>\", \"<b>Enemies</b></h3>\", 'Friend', nodes, links, degree, visited)\n",
    "        # Extract for Enemies\n",
    "        find_people(page, name, \"<b>Enemies</b></h3>\", \"Series information</h2>\", 'Enemy', nodes, links, degree, visited)\n",
    "    except:\n",
    "        break\n",
    "    iterator += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
